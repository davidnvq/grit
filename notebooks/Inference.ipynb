{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a158a3-f5ce-4f3b-a15a-b141bfd636ee",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "\n",
    "(**Please modify it accordingly**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3bc0ec3-359c-4a8d-8e92-522568d04dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'COCO_val2014_000000000772.jpg'\n",
    "vocab_path = '/home/quang/datasets/coco_caption/annotations/vocab.json'\n",
    "checkpoint = \"/home/quang/checkpoints/ecaptioner/coco/exp34b/grit_checkpoint_4ds.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b4503-e167-4778-9693-3f10399eab7b",
   "metadata": {},
   "source": [
    "### Intialize a Hydra Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a503d78-fc23-4fe2-99ee-936c0f8886c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "\n",
    "# initialize hydra config\n",
    "GlobalHydra.instance().clear()\n",
    "initialize(config_path=\"../configs/caption\")\n",
    "config = compose(config_name='coco_config.yaml', overrides=[f\"exp.checkpoint={checkpoint}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8058d63e-5a58-446c-9852-668a3beca7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# model\n",
    "from models.caption.detector import build_detector\n",
    "from models.caption import Transformer\n",
    "\n",
    "# dataset\n",
    "from PIL import Image\n",
    "from datasets.caption.field import TextField\n",
    "from datasets.caption.transforms import get_transform\n",
    "from engine.utils import nested_tensor_from_tensor_list\n",
    "\n",
    "device = torch.device(f\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5ba91d-45f2-4eff-b4be-d615cd39b110",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b111134-5375-40d8-b661-f7910ea7c6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model missing: 0\n",
      "model unexpected: 0\n"
     ]
    }
   ],
   "source": [
    "detector = build_detector(config).to(device)\n",
    "model = Transformer(detector, config=config)\n",
    "model = model.to(device)\n",
    "\n",
    "# load checkpoint\n",
    "if os.path.exists(config.exp.checkpoint):\n",
    "    checkpoint = torch.load(config.exp.checkpoint, map_location='cpu')\n",
    "    missing, unexpected = model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "    print(f\"det missing:{len(missing)} det unexpected:{len(unexpected)}\")\n",
    "    \n",
    "model.cached_features = False\n",
    "\n",
    "# prepare utils\n",
    "transform = get_transform(config.dataset.transform_cfg)['valid']\n",
    "text_field = TextField(vocab_path=vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80123572-55ae-45b7-a0c5-d6c98630c2f4",
   "metadata": {},
   "source": [
    "### Load and Transform An Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ba6f23e-429c-4145-97a2-e2ea8d17c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_image = Image.open(img_path).convert('RGB')\n",
    "image = transform(rgb_image)\n",
    "images = nested_tensor_from_tensor_list([image]).to(device)\n",
    "# rgb_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffd4556-d2f5-4a07-badb-2a204e25c692",
   "metadata": {},
   "source": [
    "### Inference and Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85ef635b-3789-425b-9a95-a75f0f1e4b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two sheep standing next to a fence in the grass\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    pred_tokens, _ = model(images,                   \n",
    "                   seq=None,\n",
    "                   use_beam_search=True,\n",
    "                   max_len=config.model.beam_len,\n",
    "                   eos_idx=config.model.eos_idx,\n",
    "                   beam_size=config.model.beam_size,\n",
    "                   out_size=1,\n",
    "                   return_probs=False,\n",
    "                  )\n",
    "    caption = text_field.decode(pred_tokens, join_words=True)[0]                                                \n",
    "    print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5652d5f0-90f8-4a2d-88a3-5613074831fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
